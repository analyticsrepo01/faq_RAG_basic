import {
  __async,
  __spreadProps,
  __spreadValues
} from "./chunk-WFI2LP4G.mjs";
import {
  defineModel,
  GenerationCommonConfigSchema,
  getBasicUsageStats,
  modelRef
} from "@genkit-ai/ai/model";
import z from "zod";
import { predictModel } from "./predict.js";
const ImagenConfigSchema = GenerationCommonConfigSchema.extend({
  /** Language of the prompt text. */
  language: z.enum(["auto", "en", "es", "hi", "ja", "ko", "pt", "zh-TW", "zh", "zh-CN"]).optional(),
  /** Desired aspect ratio of output image. */
  aspectRatio: z.enum(["1:1", "9:16", "16:9"]).optional(),
  /** A negative prompt to help generate the images. For example: "animals" (removes animals), "blurry" (makes the image clearer), "text" (removes text), or "cropped" (removes cropped images). */
  negativePrompt: z.string().optional(),
  /** Any non-negative integer you provide to make output images deterministic. Providing the same seed number always results in the same output images. Accepted integer values: 1 - 2147483647. */
  seed: z.number().optional()
});
const imagen2 = modelRef({
  name: "vertexai/imagen2",
  info: {
    label: "Vertex AI - Imagen2",
    supports: {
      media: false,
      multiturn: false,
      tools: false,
      systemRole: false,
      output: ["media"]
    }
  },
  configSchema: ImagenConfigSchema
});
function extractText(request) {
  return request.messages.at(-1).content.map((c) => c.text || "").join("");
}
function toParameters(request) {
  var _a, _b, _c, _d, _e;
  const out = {
    sampleCount: (_a = request.candidates) != null ? _a : 1,
    aspectRatio: (_b = request.config) == null ? void 0 : _b.aspectRatio,
    negativePrompt: (_c = request.config) == null ? void 0 : _c.negativePrompt,
    seed: (_d = request.config) == null ? void 0 : _d.seed,
    language: (_e = request.config) == null ? void 0 : _e.language
  };
  for (const k in out) {
    if (!out[k])
      delete out[k];
  }
  return out;
}
function extractPromptImage(request) {
  var _a, _b, _c;
  return (_c = (_b = (_a = request.messages.at(-1)) == null ? void 0 : _a.content.find((p) => !!p.media)) == null ? void 0 : _b.media) == null ? void 0 : _c.url.split(",")[1];
}
function imagen2Model(client, options) {
  const predict = predictModel(client, options, "imagegeneration@005");
  return defineModel(
    __spreadProps(__spreadValues({
      name: imagen2.name
    }, imagen2.info), {
      configSchema: ImagenConfigSchema
    }),
    (request) => __async(this, null, function* () {
      const instance = {
        prompt: extractText(request)
      };
      if (extractPromptImage(request))
        instance.image = { bytesBase64Encoded: extractPromptImage(request) };
      const req = {
        instances: [instance],
        parameters: toParameters(request)
      };
      const response = yield predict([instance], toParameters(request));
      const candidates = response.predictions.map((p, i) => {
        const b64data = p.bytesBase64Encoded;
        const mimeType = p.mimeType;
        return {
          index: i,
          finishReason: "stop",
          message: {
            role: "model",
            content: [
              {
                media: {
                  url: `data:${mimeType};base64,${b64data}`,
                  contentType: mimeType
                }
              }
            ]
          }
        };
      });
      return {
        candidates,
        usage: __spreadProps(__spreadValues({}, getBasicUsageStats(request.messages, candidates)), {
          custom: { generations: candidates.length }
        }),
        custom: response
      };
    })
  );
}
export {
  imagen2,
  imagen2Model
};
//# sourceMappingURL=imagen.mjs.map